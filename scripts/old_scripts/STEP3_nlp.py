import warnings
warnings.filterwarnings('ignore')
from pathlib import Path
import pickle
import re
from tqdm import tqdm
tqdm.pandas()
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import copy
pd.options.display.max_columns = None
pd.options.display.max_colwidth = None
import nltk
from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.manifold import TSNE
import chart_studio.plotly as py
import plotly.graph_objects as go
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics import silhouette_score
from sklearn.decomposition import NMF, LatentDirichletAllocation
from sklearn.datasets import fetch_20newsgroups
from sklearn.pipeline import Pipeline
from sklearn import metrics
vectorizer = TfidfVectorizer()
from sklearn import decomposition
from sklearn.cluster import KMeans
from sklearn.decomposition import TruncatedSVD
from scipy.spatial.distance import cdist
from scipy.sparse import random as sparse_random
from collections import Counter
import random
import skfuzzy as fuzz
from fcmeans import FCM
import spacy
from spacy import displacy
nlp = spacy.load('en_core_web_sm')
from spacy.lang.en.stop_words import STOP_WORDS
stopwords = list(STOP_WORDS)
import functools

class nlp_pipeline():
    """
    the input dataframe is by default appid index
    df is the MERGED dataframe
    df_labels is the output of self.kmeans_cluster and saved with name 'predicted_kmeans_labels
    03/08/21
        After eyeballing self.eyeball_app_changed_niche_label(), I see that they do not ever change over time, the description.
        So I decide from now on to combine all panel's description together.

    03/09/21
        I found that those appids are with exactly the same text information in each panel (after deleting None and impute missing).
        So I conclude that the apps that changed from niche to broad type is completely due to the algorithm only.
        It is pointless to conduct panel specific text label prediction.
        From this particular method, I prove that apps' text labels are time INVARIANT.
        Now I should find a suitable panel regression model that incorporate time-invariant independent variables.

    04/02/21
        1.  I have deleted combined_panel, because I have checked that descriptions in every panel is the same,
            and it is time-invariant, so niche label will be generated by combining all panels.
        2.  I will delete consecutive_panel = True, and uses all panels' text columns in generating niche label,
            this is because I will be using difference-in-difference for pre and after covid, therefore I need to use all columns.
            Even if you are not using diff-in-diff, since niche label is time-invariant, so using all panels will not be any different
            from using only consecutive panels.
        3.  I have deleted functions that assign niche dummy or niche scale dummy, and descriptive stats functions
            relates to niche labels because they are moved to regression_analysis class
        4.  I will divide full sample into game and nongame, and do the NLP algorithm within each subsample.
            Note that game and nongame are time-invariant.
            Accordingly, I will delete the function that generate genreIdGame in regression_analysis class, so
            this dummy is only generated once in the entire code.
    """
    nlp_graph_path = Path(
        '/home/naixin/Insync/naixin88@sina.cn/OneDrive/__CODING__/PycharmProjects/GOOGLE_PLAY/___essay_1___/nlp/graphs')
    nlp_stats_path = Path(
        '/home/naixin/Insync/naixin88@sina.cn/OneDrive/__CODING__/PycharmProjects/GOOGLE_PLAY/___essay_1___/nlp/stats')
    panel_path = Path(
        '/home/naixin/Insync/naixin88@sina.cn/OneDrive/_____GWU_ECON_PHD_____/___Dissertation___/____WEB_SCRAPER____/__PANELS__/___essay_1_panels___')
    tokenizer = nlp.Defaults.create_tokenizer(nlp)

    def __init__(self,
                 tcn,
                 initial_panel,
                 all_panels,
                 df=None,
                 ssvard=None,
                 sub_sample_text_cols=None,
                 tf_idf_matrices=None,
                 optimal_svd_dict=None,
                 svd_matrices=None,
                 optimal_k_cluster_dict=None,
                 output_labels=None):
        self.tcn = tcn
        self.initial_panel = initial_panel
        self.all_panels = all_panels
        self.df = df
        self.ssvard = ssvard
        self.ss_text_cols = sub_sample_text_cols
        self.tf_idf_matrices = tf_idf_matrices
        self.optimal_svd_dict = optimal_svd_dict
        self.svd_matrices = svd_matrices
        self.optimal_k_cluster_dict = optimal_k_cluster_dict
        self.output_labels = output_labels

    def open_divided_df(self):
        """
        The reason to use imputed missing dataframe instead of imputed and deleted missing is that everytime you delete different number of rows
        (depends on the definition of missig), but not every time you need to re-run text clustering (because it is time-consuming),
        so I will just use the FULL converted data and you could merged this predicted labels to imputed and deleted missing in combine_dataframes class.
        """
        f_name = self.initial_panel + '_imputed_deleted_subsamples.pickle'
        q = nlp_pipeline.panel_path / f_name
        with open(q, 'rb') as f:
            self.df = pickle.load(f)
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def create_subsample_slice_vars(self):
        """
        Purpose of creating this cell is to avoid creating run_subsample switch in each function below.
        Everytime you run the full sample NLP, you run the sub samples NLP simultaneously, it would take longer, but anyways.
        """
        app_categories = self.df['ImputedgenreId_Mode'].unique().tolist()
        self.ssvard = {'full': ['full'],
                       'minInstalls':    ['Tier1', 'Tier2', 'Tier3'],
                       'genreId': app_categories,
                        'categories':    ['category_GAME',
                                          'category_BUSINESS',
                                          'category_SOCIAL',
                                          'category_LIFESTYLE',
                                          'category_MEDICAL'],
                       'starDeveloper':  ['top_digital_firms',
                                          'non-top_digital_firms']}
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

        # ----------------------------- subset the text column for each sub-sample -------------------------------------
    def slice_text_cols_for_sub_samples(self):
        d = dict.fromkeys(self.ssvard)
        for k, sub_samples in self.ssvard.items():
            d[k] = dict.fromkeys(sub_samples)
            for i in sub_samples:
                if i == 'full':
                    d[k][i] = self.df[self.tcn + 'ModeClean'].copy(deep=True)
                else:
                    d[k][i] = self.df.loc[self.df[i] == 1, [self.tcn + 'ModeClean']].squeeze().copy(deep=True)
        self.ss_text_cols = d
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)


    def tf_idf_transformation(self):
        pipe = Pipeline(steps=[('tfidf',
                                TfidfVectorizer(
                                    stop_words='english',
                                    strip_accents='unicode',
                                    max_features=1500))])
        matrix_df_dict = dict.fromkeys(self.ss_text_cols.keys())
        for sample, content in matrix_df_dict.items():
            matrix_df_dict[sample] = dict.fromkeys(self.ss_text_cols[sample].keys())
        for sample, content in self.ss_text_cols.items():
            for ss_name, col in content.items():
                print('TF-IDF TRANSFORMATION')
                print(self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                matrix = pipe.fit_transform(col)
                matrix_df = pd.DataFrame(matrix.toarray(),
                                         columns=pipe['tfidf'].get_feature_names())
                matrix_df['app_ids'] = col.index.tolist()
                matrix_df.set_index('app_ids', inplace=True)
                matrix_df_dict[sample][ss_name] = matrix_df
                print(matrix_df.shape)
        self.tf_idf_matrices = matrix_df_dict
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def find_optimal_svd_component_plot(self):
        """
        https://medium.com/swlh/truncated-singular-value-decomposition-svd-using-amazon-food-reviews-891d97af5d8d
        https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html
        """
        for sample, content in self.tf_idf_matrices.items():
            for ss_name, matrix in content.items():
                print('FIND OPTIMAL SVD COMPONENTS')
                print(self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                n_comp = np.round(np.linspace(0, matrix.shape[1]-1, 20))
                n_comp = n_comp.astype(int)
                explained = []
                for x in tqdm(n_comp):
                    svd = TruncatedSVD(n_components=x)
                    svd.fit(matrix)
                    explained.append(svd.explained_variance_ratio_.sum())
                    print("Number of components = %r and explained variance = %r" % (x, svd.explained_variance_ratio_.sum()))
                fig, ax = plt.subplots()
                ax.plot(n_comp, explained)
                ax.grid()
                plt.xlabel('Number of components')
                plt.ylabel("Explained Variance")
                plt.title(self.initial_panel + sample + ss_name + " Plot of Number of components v/s explained variance")
                filename = self.initial_panel + '_' + sample + '_' + ss_name + '_optimal_svd_graph.png'
                fig.savefig(nlp_pipeline.nlp_graph_path / 'optimal_svd_comp' / filename, facecolor='white', dpi=300)
                plt.show()
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def find_optimal_svd_component_dict(self, cutoff_percent_explained):
        d = dict.fromkeys(self.ssvard)
        for k, ss in self.ssvard.items():
            d[k] = dict.fromkeys(ss)
            for i in ss:
                print('FIND OPTIMAL SVD COMPONENTS')
                matrix = self.tf_idf_matrices[k][i]
                n_comp = np.round(np.linspace(0, matrix.shape[1] - 1, 40))
                n_comp = n_comp.astype(int)
                x = 0
                while x <= len(n_comp)-1:
                    svd = TruncatedSVD(n_components=n_comp[x])
                    svd.fit(matrix)
                    print(self.initial_panel, ' -- ', k, ' -- ', i)
                    print('Number of Components: ', n_comp[x])
                    print('Explained Variance Ratio: ', svd.explained_variance_ratio_.sum())
                    if svd.explained_variance_ratio_.sum() < cutoff_percent_explained:
                        x += 1 # continue the while loop to test next ncomp
                    else:
                        d[k][i] = n_comp[x]
                        print("The Optimal SVD Component is = %r and the explained variance = %r" % (
                        n_comp[x], svd.explained_variance_ratio_.sum()))
                        x = len(n_comp) # set the x value so to break the while loop
        self.optimal_svd_dict = d
        # ----------------- save -----------------------------------------------
        filename = self.initial_panel + '_optimal_svd_dict.pickle'
        q = nlp_pipeline.nlp_stats_path / filename
        pickle.dump(d, open(q, 'wb'))
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def truncate_svd(self, random_state):
        f_name = self.initial_panel + '_optimal_svd_dict.pickle'
        q = nlp_pipeline.nlp_stats_path / f_name
        with open(q, 'rb') as f:
            self.optimal_svd_dict = pickle.load(f)
        # -------------------------------------------------------------------------
        matrix_df_dict = dict.fromkeys(self.ss_text_cols.keys())
        for sample, content in matrix_df_dict.items():
            matrix_df_dict[sample] = dict.fromkeys(self.ss_text_cols[sample].keys())
        for sample, content in self.tf_idf_matrices.items():
            for ss_name, matrix in content.items():
                print('TRUNCATE SVD')
                print(self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                svd = TruncatedSVD(n_components=self.optimal_svd_dict[sample][ss_name],
                                   random_state=random_state)
                matrix_transformed = svd.fit_transform(matrix)
                print(matrix_transformed.shape)
                matrix_transformed_df = pd.DataFrame(matrix_transformed)  # do not need to assign column names because those do not correspond to each topic words (they have been transformed)
                matrix_transformed_df['app_ids'] = matrix.index.tolist()
                matrix_transformed_df.set_index('app_ids', inplace=True)
                matrix_df_dict[sample][ss_name] = matrix_transformed_df
        self.svd_matrices = matrix_df_dict
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def optimal_k_elbow(self, type):
        """
        https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f
        https://scikit-learn.org/stable/modules/clustering.html
        https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/
        1. Distortion: It is calculated as the average of the squared distances from the cluster centers of the respective clusters.
           Typically, the Euclidean distance metric is used.
        2. Inertia: It is the sum of squared distances of samples to their closest cluster center.
        type is whether 'distortion' or 'inertia'
        """
        for sample, content in self.svd_matrices.items():
            for ss_name, matrix in content.items():
                print(self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                n_cluster_list = np.round(np.linspace(1, matrix.shape[0] - 0.8 * matrix.shape[0], 10))
                n_cluster_list = n_cluster_list.astype(int)
                metrics = []
                metrics_dict = {}
                for k in tqdm(n_cluster_list):
                    km = KMeans(n_clusters=k)
                    km = km.fit(matrix)
                    if type == 'distortion':
                        distortion = sum(np.min(cdist(matrix,
                                                        km.cluster_centers_,
                                                        'euclidean'), axis=1)) / matrix.shape[0]
                        metrics.append(distortion)
                        metrics_dict[k] = distortion
                        print('DISTORTION -- ', self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                        print(k, distortion)
                        y_label = 'Distortions'
                        title = self.initial_panel + sample + ss_name + ' Elbow Method (Distortion)'
                        f_name = self.initial_panel + '_' + sample + '_' + ss_name + '_elbow_distortion.png'
                        dict_f_name = self.initial_panel + '_' + sample + '_' + ss_name + '_elbow_distortion_dict.pickle'
                    elif type == 'inertia':
                        metrics.append(km.inertia_)
                        metrics_dict[k] = km.inertia_
                        print('INERTIA -- ', self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                        print(k, km.inertia_)
                        y_label = 'Inertia'
                        title = self.initial_panel + sample + ss_name + ' Elbow Method (Inertia)'
                        f_name = self.initial_panel + '_' + sample + '_' + ss_name + '_elbow_inertia.png'
                        dict_f_name = self.initial_panel + '_' + sample + '_' + ss_name + '_elbow_inertia_dict.pickle'
                fig, ax = plt.subplots()
                ax.plot(n_cluster_list, metrics, 'bx-')
                ax.grid()
                plt.xlabel('k')
                plt.ylabel(y_label)
                plt.title(title)
                fig.savefig(nlp_pipeline.nlp_graph_path / 'optimal_clusters' / f_name, facecolor='white', dpi=300)
                plt.show()
                # ----------------- save -----------------------------------------------
                q = nlp_pipeline.nlp_stats_path / dict_f_name
                pickle.dump(metrics_dict, open(q, 'wb'))
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def determine_optimal_k_from_elbow(self, type):
        """
        :param type: 'distortion' or 'inertia'
        :return:
        """
        df_list = []
        for sample_name1, content in self.ssvard.items():
            for sample_name2 in content:
                f_name = self.initial_panel + '_' + sample_name1 + '_' + sample_name2 + '_elbow_' + type + '_dict.pickle'
                q = nlp_pipeline.nlp_stats_path / f_name
                with open(q, 'rb') as f:
                    d = pickle.load(f)
                    df = pd.DataFrame(d, index=[0]) # https://stackoverflow.com/questions/17839973/constructing-pandas-dataframe-from-values-in-variables-gives-valueerror-if-usi
                    df = df.T
                    df_list.append(df)
        return df_list

    def optimal_k_silhouette(self):
        """
        https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb
        https://medium.com/@kunal_gohrani/different-types-of-distance-metrics-used-in-machine-learning-e9928c5e26c7
        I think it is just better off using sihouette score because global maximum is better to find out than the elbow point.
        The Silhouette Coefficient is calculated using the mean intra-cluster distance (a)
        and the mean nearest-cluster distance (b) for each sample. The Silhouette Coefficient for a sample is (b - a) / max(a, b).
        To clarify, b is the distance between a sample and the nearest cluster that the sample is not a part of.
        Note that Silhouette Coefficient is only defined if number of labels is 2 <= n_labels <= n_samples - 1.
        """
        d = dict.fromkeys(self.ssvard.keys())
        for sample, content in self.svd_matrices.items():
            d[sample] = dict.fromkeys(content.keys())
            for ss_name, matrix in content.items():
                # starting from 2 because this score need to calculate between cluster estimators
                n_cluster_list = np.round(np.linspace(2, matrix.shape[0] - 0.9 * matrix.shape[0], 10))
                n_cluster_list = n_cluster_list.astype(int)
                silhouette_scores = []
                silhouette_scores_dict = {}
                print('SILHOUETTE SCORE -- ', self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                for k in tqdm(n_cluster_list):
                    km = KMeans(n_clusters=k)
                    km = km.fit(matrix)
                    labels = km.labels_
                    s_score = silhouette_score(matrix, labels, metric='cosine')
                    silhouette_scores.append(s_score)
                    silhouette_scores_dict[k] = s_score
                d[sample][ss_name] = silhouette_scores_dict
                fig, ax = plt.subplots()
                ax.plot(n_cluster_list, silhouette_scores, 'bx-')
                ax.grid()
                plt.xlabel('k')
                plt.ylabel('silhouette_scores (cosine distance)')
                plt.title(self.initial_panel + sample + ss_name + ' Silhouette Scores For Optimal k')
                filename = self.initial_panel + '_' + sample + '_' + ss_name + '_silhouette_optimal_cluster.png'
                fig.savefig(nlp_pipeline.nlp_graph_path / 'optimal_clusters' / filename, facecolor='white', dpi=300)
                plt.show()
        # ----------------- save -----------------------------------------------
        dict_f_name = self.initial_panel + '_silhouette_score_dict.pickle'
        q = nlp_pipeline.nlp_stats_path / dict_f_name
        pickle.dump(d, open(q, 'wb'))
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def determine_optimal_k_from_silhouette(self):
        dict_f_name = self.initial_panel + '_silhouette_score_dict.pickle'
        q = nlp_pipeline.nlp_stats_path / dict_f_name
        with open(q, 'rb') as f:
            res = pickle.load(f)
        d = dict.fromkeys(self.ssvard.keys())
        for sample1, content in self.ssvard.items():
            d[sample1] = dict.fromkeys(content)
            for sample2 in content:
                df = copy.deepcopy(res[sample1][sample2])
                df2 = pd.DataFrame(df, index=[0])
                df3 = df2.T
                optimal_k = df3.idxmax(axis=0)
                print(self.initial_panel, ' -- ', sample1, ' -- ', sample2, ' -- ', ' Optimal K From Global Max of Silhouette Score')
                print(optimal_k)
                print()
                d[sample1][sample2] = optimal_k
        self.optimal_k_cluster_dict = d
        # ----------------- save -----------------------------------------------
        dict_f_name = self.initial_panel + '_optimal_k_from_global_max_of_silhouette_score.pickle'
        q = nlp_pipeline.nlp_stats_path / dict_f_name
        pickle.dump(d, open(q, 'wb'))
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def open_optimal_k(self):
        dict_f_name = self.initial_panel + '_optimal_k_from_global_max_of_silhouette_score.pickle'
        q = nlp_pipeline.nlp_stats_path / dict_f_name
        with open(q, 'rb') as f:
            self.optimal_k_cluster_dict = pickle.load(f)
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)

    def kmeans_cluster(self,
                       random_state):
        label_dict = dict.fromkeys(self.svd_matrices.keys())
        for sample, content in self.svd_matrices.items():
            label_dict[sample] = dict.fromkeys(self.svd_matrices[sample].keys())
            for ss_name, matrix in content.items():
                print('KMEANS CLUSTER')
                print(self.initial_panel, ' -- ', sample, ' -- ', ss_name)
                print('input matrix shape')
                print(matrix.shape)
                print('optimal k clusters')
                k = self.optimal_k_cluster_dict[sample][ss_name]
                print(k)
                y_kmeans = KMeans(
                        n_clusters=int(k),
                        random_state=random_state
                    ).fit_predict(
                        matrix
                    )  # it is equivalent as using fit then .label_.
                matrix[sample + '_' + ss_name + '_kmeans_labels'] = y_kmeans
                label_single = matrix[[sample + '_' + ss_name + '_kmeans_labels']]
                label_dict[sample][ss_name] = label_single
        self.output_labels = label_dict
        # --------------------------- save -------------------------------------------------
        # for this one, you do not need to run text cluster label every month when you scraped new data, because they would more or less stay the same
        filename = self.initial_panel + '_predicted_labels_dict.pickle'
        q = nlp_pipeline.panel_path / 'predicted_text_labels' / filename
        pickle.dump(self.output_labels, open(q, 'wb'))
        return nlp_pipeline(
                 tcn=self.tcn,
                 initial_panel=self.initial_panel,
                 all_panels=self.all_panels,
                 df=self.df,
                 ssvard=self.ssvard,
                 sub_sample_text_cols=self.ss_text_cols,
                 tf_idf_matrices=self.tf_idf_matrices,
                 optimal_svd_dict=self.optimal_svd_dict,
                 svd_matrices=self.svd_matrices,
                 optimal_k_cluster_dict=self.optimal_k_cluster_dict,
                 output_labels=self.output_labels)
